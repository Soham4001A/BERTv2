# Instantiates a (non-huggingface) scriptable encoder-based LM with BERT as baseline
# Modernized version of bert-c5

# Useful commands for compile cache removel during studies-
# find . -name "__pycache__" -type d -exec rm -rf {} +
# find . -name "*.pyc" -delete
# rm -rf ~/.cache/torch/compile

# These are the huggingface bert parameters
architectures:
  - ScriptableCrammedBERT

num_transformer_layers: 16 # --> This is really rather the number of transformer blocks
hidden_size: 768
intermed_size: 3072
hidden_dropout_prob: 0.1

norm: LayerNorm
norm_eps: 1e-12
norm_scheme: pre #"pre" is baked into the new implementation
nonlin: GELUglu

tie_weights: True # Tie input/output embedding
decoder_bias: False # Whether to include a bias in the decoding step

sparse_prediction: ${train.objective.mlm_probability} # Whether to predict only on masked tokens, and how many there will be
loss: cross-entropy
objective_layout: MLM # can also be SCRIPT

embedding:
  vocab_size: # will be populated automatically
  pos_embedding: scaled-sinusoidal
  dropout_prob: 0.1 # equal to hidden_dropout_prob in BERT
  pad_token_id: 0
  max_seq_length: 128 # max seq length that the positional embedding is instantiated for
  embedding_dim: ${arch.hidden_size} # has to be this value for crammedBERT
  normalization: True
  stable_low_precision: False

attention:
  # ───────────── core switch ─────────────
  type: self-attention      # ← enable standard self-attention
  num_attention_heads: 12     # for standard self-attention

  use_outer_ffn: True # whether to use the outer FFN in the attention block (only for MHA)
  # ───────────── LMA switch ─────────────

  # type: lma                # ← enable Latent Meta Attention
  # num_heads: 12           # number of attention heads
  causal_attention: True  # LMA is encoder‑style (leave this False)


  # ─── head stacking before the latent step ───
  num_heads_stacking: 12    # how many standard heads to “stack” into one block
  qkv_bias: False
  dropout_prob: 0.1
  skip_output_projection: False  # keep the output Linear

  # ───────────── latent‑space hyper‑params ─────────────
  d_new: 384               # size of each latent token (≈ hidden_size / 2)
  target_l_new: 64         # desired latent sequence length (≈ S / 2)
  static_seq_len: 128      # pre‑build LMA once for this sequence length
  num_heads_latent: 12     # heads inside each latent block
  num_blocks: 1            # how many latent transformer blocks per layer (we need better convetion for this)
  # ff_latent_hidden: 1536   # inner size of the latent MLP (≈ 4 × d_new) (maybe?)
  ff_latent_hidden: 2304
  #ff_latent_hidden: 3072   # lets try to keep this the same as normal BERT

  # ───────────── misc options ─────────────
  rotary_embedding: False
  seq_op_in_fp32: False
  sequence_op: torch-softmax
init:
  type: normal
  std: 0.02

# Experimental options:
ffn_layer_frequency: 1 # FFN layer in every layer
skip_head_transform: True # This is only possible if embedding_dim=hidden_size
use_bias: False # Whether to learn biases on all dense layers
final_norm: True # Add a final norm layer before the end

# Downstream settings:
num_labels: # This can be automatically filled in for downstream
classification_head:
  pooler: avg
  include_ff_layer: True
  head_dim: 1024
  nonlin: Tanh
  classifier_dropout: ${arch.hidden_dropout_prob}
